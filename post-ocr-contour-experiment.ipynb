{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "sys.path.append('/lib/x86_64-linux-gnu')\n",
    "sys.path.append('/')    \n",
    "import json, base64, binascii,hashlib\n",
    "from evernote.api.client import EvernoteClient\n",
    "import evernote.edam.type.ttypes as Types\n",
    "import evernote.edam.error.ttypes as Errors\n",
    "\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "main_path = \"\"\"C:\\\\Users\\\\Shaham Farooq\\\\Documents\\\\MyProjects\\\\digilight\\\\\"\"\"\n",
    "from config import GOOGLE_API_KEY\n",
    "from config import SECRET_KEY, EVERNOTE_DEV_TOKEN, EN_CONSUMER_KEY, EN_CONSUMER_SECRET, DEBUG\n",
    "\n",
    "DETECTION_TYPES = [\n",
    "    'TYPE_UNSPECIFIED',\n",
    "    'FACE_DETECTION',\n",
    "    'LANDMARK_DETECTION',\n",
    "    'LOGO_DETECTION',\n",
    "    'LABEL_DETECTION',\n",
    "    'TEXT_DETECTION',\n",
    "    'SAFE_SEARCH_DETECTION',\n",
    "    'DOCUMENT_TEXT_DETECTION'\n",
    "]\n",
    "\n",
    "def get_evernote_client(token=None):\n",
    "    if token:\n",
    "        return EvernoteClient(token=token, sandbox=DEBUG)\n",
    "    else:\n",
    "        return EvernoteClient(\n",
    "            consumer_key=EN_CONSUMER_KEY,\n",
    "            consumer_secret=EN_CONSUMER_SECRET,\n",
    "            sandbox=DEBUG\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_img_to_json(input_file):\n",
    "    \"\"\"Translates the input file into a json output file.\n",
    "\n",
    "    Args:\n",
    "        input_file: a file object, containing lines of input to convert.\n",
    "        output_json: the json request to send to API\n",
    "    \"\"\"\n",
    "    request_list = []\n",
    "    for line in input_file:\n",
    "        image_filename, features = line.lstrip().split(' ', 1)\n",
    "\n",
    "        with open(image_filename, 'rb') as image_file:\n",
    "            content_json_obj = {\n",
    "                'content': base64.b64encode(image_file.read()).decode('UTF-8')\n",
    "            }\n",
    "\n",
    "        feature_json_obj = []\n",
    "        for word in features.split(' '):\n",
    "            feature, max_results = word.split(':', 1)\n",
    "            feature_json_obj.append({\n",
    "                'type': get_detection_type(feature),\n",
    "                'maxResults': int(max_results),\n",
    "            })\n",
    "\n",
    "        request_list.append({\n",
    "            'features': feature_json_obj,\n",
    "            'image': content_json_obj,\n",
    "        })\n",
    "\n",
    "    return {'requests': request_list}\n",
    "\n",
    "\n",
    "\n",
    "def get_detection_type(detect_num):\n",
    "    \"\"\"Return the Vision API symbol corresponding to the given number.\"\"\"\n",
    "    detect_num = int(detect_num)\n",
    "    if 0 < detect_num < len(DETECTION_TYPES):\n",
    "        return DETECTION_TYPES[detect_num]\n",
    "    else:\n",
    "        return DETECTION_TYPES[0]\n",
    "\n",
    "def custom_smoothen_cnt(cnt):\n",
    "    M = cv2.moments(cnt)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    "    leftmost = tuple(cnt[cnt[:, :, 0].argmin()][0])\n",
    "    rightmost = tuple(cnt[cnt[:, :, 0].argmax()][0])\n",
    "    topmost = tuple(cnt[cnt[:, :, 1].argmin()][0])\n",
    "    bottommost = tuple(cnt[cnt[:, :, 1].argmax()][0])\n",
    "    print \"Edge Points:\", leftmost, rightmost, topmost, bottommost\n",
    "    area = cv2.contourArea(cnt)\n",
    "    print \"AREA of Orig Contour:\", area\n",
    "    newcnt = cnt.copy()\n",
    "    for i in range(cnt.shape[0]):\n",
    "        x = cnt[i, 0, 0]\n",
    "        y = cnt[i, 0, 1]\n",
    "        if x>leftmost[0] and y<leftmost[1] and x<cx:\n",
    "            newcnt[i, 0, 0] = leftmost[0]\n",
    "        elif x<rightmost[0] and y<leftmost[1] and x<cx:\n",
    "            newcnt[i, 0, 0] = rightmost[0]\n",
    "    area = cv2.contourArea(newcnt)\n",
    "    print \"AREA of Smoothened Contour:\", area\n",
    "    return newcnt\n",
    "\n",
    "def smoothen_contour(contour):\n",
    "    from scipy.interpolate import splprep, splev\n",
    "    x, y = contour.T\n",
    "    # Convert from numpy arrays to normal arrays\n",
    "    x = x.tolist()[0]\n",
    "    y = y.tolist()[0]\n",
    "    # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.splprep.html\n",
    "    tck, u = splprep([x, y], u=None, s=15.0, per=1)\n",
    "    # https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linspace.html\n",
    "    u_new = np.linspace(u.min(), u.max(), len(contour)*0.05)\n",
    "    # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.splev.html\n",
    "    x_new, y_new = splev(u_new, tck, der=0)\n",
    "    # Convert it back to numpy format for opencv to be able to display it\n",
    "    res_array = [[[int(i[0]), int(i[1])]] for i in zip(x_new, y_new)]\n",
    "    smooth_cnt = np.asarray(res_array, dtype=np.int32)\n",
    "    return smooth_cnt\n",
    "\n",
    "def expand_contour(cnt,expand_rate_x=1.5,expand_rate_y=1.5):\n",
    "    M = cv2.moments(cnt)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    "    leftmost = tuple(cnt[cnt[:, :, 0].argmin()][0])\n",
    "    rightmost = tuple(cnt[cnt[:, :, 0].argmax()][0])\n",
    "    topmost = tuple(cnt[cnt[:, :, 1].argmin()][0])\n",
    "    bottommost = tuple(cnt[cnt[:, :, 1].argmax()][0])\n",
    "    print \"Edge Points:\", leftmost, rightmost, topmost, bottommost\n",
    "    print \"Centroid: (\",cx,\",\",cy,\")\"\n",
    "    area = cv2.contourArea(cnt)\n",
    "    print \"AREA of Contour:\",area\n",
    "    newcnt = cnt.copy()\n",
    "    for i in range(cnt.shape[0]):\n",
    "        if abs(cnt[i, 0, 0] - cx)>(abs(leftmost[0]-cx)*0.8) or abs(cnt[i, 0, 1] - cy) > (abs(topmost[1] - cy) * 0.8):\n",
    "            newcnt[i, 0, 0] = ((cnt[i, 0, 0] - cx) * expand_rate_x) + cx\n",
    "            newcnt[i, 0, 1] = ((cnt[i, 0, 1] - cy) * expand_rate_y) + cy\n",
    "\n",
    "    return newcnt\n",
    "\n",
    "def contour_img(img_path,thresh=400,std_dev=4, hsv_lower=[22, 30, 30], hsv_upper=[45, 255, 255]):\n",
    "    \"\"\"Returns the name of the saved contour PNG image that will be sent for OCR thru API\"\"\"\n",
    "    contoured_img = \"contoured_\"+os.path.basename(img_path).split(\".\")[0]+\".png\"\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # rgb to HSV color spave conversion\n",
    "    hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    HSV_lower = np.array(hsv_lower, np.uint8)  # Lower HSV value\n",
    "    HSV_upper = np.array(hsv_upper, np.uint8)  # Upper HSV value\n",
    "\n",
    "    frame_threshed = cv2.inRange(hsv_img, HSV_lower, HSV_upper)\n",
    "    # find connected components\n",
    "    _, contours, hierarchy, = cv2.findContours(frame_threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Draw contours around filtered objects\n",
    "    OutputImg = image.copy()\n",
    "    cnt_lens = [cv2.contourArea(x) for x in contours]\n",
    "\n",
    "    klist = cnt_lens\n",
    "    avglatlist = range(0,len(cnt_lens))\n",
    "\n",
    "    klist_np = np.array(klist).astype(np.float)\n",
    "    avglatlist_np = np.array(avglatlist).astype(np.float)    \n",
    "\n",
    "    # klist_filtered = klist_np[(abs(klist_np - np.mean(klist_np))) > (std_dev * np.std(klist_np))]\n",
    "    avglatlist_filtered = avglatlist_np[(abs(klist_np - np.mean(klist_np))) > (std_dev * np.std(klist_np))]\n",
    "    while len(avglatlist_filtered)==0:\n",
    "        if std_dev<1.5:\n",
    "            return False\n",
    "            break\n",
    "        std_dev=std_dev-0.5\n",
    "        avglatlist_filtered = avglatlist_np[(abs(klist_np - np.mean(klist_np))) > (std_dev * np.std(klist_np))]\n",
    "\n",
    "    max_thresh = max(cnt_lens)\n",
    "    mask = np.zeros_like(image)  # Create mask where white is what we want, black otherwise\n",
    "    out = np.zeros_like(image)  # Extract out the object and place into output image\n",
    "\n",
    "    for c in avglatlist_filtered:\n",
    "        # # remove noise objects having contour length threshold value\n",
    "        cnt = contours[int(c)]\n",
    "\n",
    "        if len(cnt) > thresh:\n",
    "            expand_cnt = expand_contour(cnt,expand_rate_x=1.02,expand_rate_y=1.04)\n",
    "            # smooth_cnt = smoothen_contour(expand_cnt)\n",
    "            epsilon = 0.001*cv2.arcLength(cnt,True)\n",
    "            print \"Epsilon\", epsilon\n",
    "            smooth_cnt = cv2.approxPolyDP(expand_cnt, epsilon, False)\n",
    "            # cust_smooth_cnt = custom_smoothen_cnt(cnt)\n",
    "            cv2.drawContours(OutputImg, [cnt], 0, (0, 0, 50), 1)\n",
    "            cv2.drawContours(OutputImg, [expand_cnt], 0, (0, 0, 255), 2)\n",
    "            cv2.drawContours(OutputImg, [smooth_cnt], 0, (255, 0, 0), 2)\n",
    "            # cv2.drawContours(OutputImg, [smooth_cnt], 0, (0, 255,0), 1)\n",
    "            cv2.drawContours(mask, [smooth_cnt], 0, (255,255,255), -1)  # Draw filled contour in mask\n",
    "            cv2.drawContours(mask, [expand_cnt], 0, (255, 255, 255), -1)  # Draw filled contour in mask\n",
    "\n",
    "\n",
    "            # hull = cv2.convexHull(cnt)\n",
    "\n",
    "    out[mask == 255] = image[mask == 255]\n",
    "    out[mask == 0] = 255\n",
    "    imgray = cv2.cvtColor(out, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #save contoured image to display\n",
    "    j = Image.fromarray(imgray)\n",
    "    j.save(contoured_img)\n",
    "\n",
    "    cv2.imwrite(\"col_\"+contoured_img,OutputImg)\n",
    "    return contoured_img\n",
    "    \n",
    "\n",
    "def google_ocr_img(img_paths):\n",
    "    \"\"\"Sends DOCUMENT_TEXT_DETECTION API request with img file as content and return OCR result\"\"\"\n",
    "    call_commands = [img_path+\" 7:5\" for img_path in img_paths]\n",
    "    data = convert_img_to_json(call_commands)\n",
    "    useragent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36'\n",
    "\n",
    "    response = requests.post(url=\"https://vision.googleapis.com/v1/images:annotate?key={key}\".format(key=GOOGLE_API_KEY),\n",
    "        data=json.dumps(data),\n",
    "        headers={'Content-Type': 'application/json',\n",
    "                'User-Agent': useragent})\n",
    "    api_result = response.json()\n",
    "    all_texts = get_all_text(api_result) \n",
    "    return api_result, \"\\n--------------------\\n\".join(all_texts)\n",
    "    \n",
    "def create_en_resource(file_list):\n",
    "    resource_list = []\n",
    "    for filename in file_list:\n",
    "        # Calculate the md5 hash of the pdf\n",
    "        md5 = hashlib.md5()\n",
    "        with open(filename, \"rb\") as imageFile:\n",
    "            file_bytes = imageFile.read()\n",
    "        md5.update(file_bytes)\n",
    "        md5hash = md5.hexdigest()\n",
    "\n",
    "        # Create the Data type for evernote that goes into a resource\n",
    "        file_data = Types.Data()\n",
    "        file_data.bodyHash = md5hash\n",
    "        file_data.size = len(file_bytes)\n",
    "        file_data.body = file_bytes\n",
    "\n",
    "        # Create a resource for the note that contains the pdf\n",
    "        file_resource = Types.Resource()\n",
    "        file_resource.data = file_data\n",
    "        file_resource.mime = \"image/jpg\"\n",
    "\n",
    "        # Create a resource list to hold the pdf resource\n",
    "        resource_list.append(file_resource)\n",
    "    return resource_list\n",
    "\n",
    "def makeNote(authToken, noteStore, noteTitle, noteBody, resources=[], parentNotebook=None):\n",
    "    \"\"\"\n",
    "    Create a Note instance with title and body\n",
    "    Send Note object to user's account\n",
    "    \"\"\"\n",
    "\n",
    "    ourNote = Types.Note()\n",
    "    ourNote.title = noteTitle\n",
    "\n",
    "    ## Build body of note\n",
    "\n",
    "    nBody = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\"\n",
    "    nBody += \"<!DOCTYPE en-note SYSTEM \\\"http://xml.evernote.com/pub/enml2.dtd\\\">\"\n",
    "    nBody += \"<en-note>%s\" % noteBody.replace(\"\\n\",\"<br />\")\n",
    "    if resources:\n",
    "        ### Add Resource objects to note body\n",
    "        nBody += \"<br />\" * 2\n",
    "        ourNote.resources = resources\n",
    "        for resource in resources:\n",
    "            hexhash = resource.data.bodyHash\n",
    "            nBody += \"Attachment with hash %s: <br /><en-media type=\\\"%s\\\" hash=\\\"%s\\\" /><br />\" % \\\n",
    "                (hexhash, resource.mime, hexhash)\n",
    "    nBody += \"</en-note>\"\n",
    "\n",
    "    ourNote.content = nBody\n",
    "\n",
    "    ## parentNotebook is optional; if omitted, default notebook is used\n",
    "    if parentNotebook and hasattr(parentNotebook, 'guid'):\n",
    "        ourNote.notebookGuid = parentNotebook.guid\n",
    "\n",
    "    ## Attempt to create note in Evernote account\n",
    "    try:\n",
    "        note = noteStore.createNote(authToken, ourNote)\n",
    "    except Errors.EDAMUserException, edue:\n",
    "        ## Something was wrong with the note data\n",
    "        ## See EDAMErrorCode enumeration for error code explanation\n",
    "        ## http://dev.evernote.com/documentation/reference/Errors.html#Enum_EDAMErrorCode\n",
    "        print \"EDAMUserException:\", edue\n",
    "        return None\n",
    "    except Errors.EDAMNotFoundException, ednfe:\n",
    "        ## Parent Notebook GUID doesn't correspond to an actual notebook\n",
    "        print \"EDAMNotFoundException: Invalid parent notebook GUID\"\n",
    "        return None\n",
    "    ## Return created note object\n",
    "    return note\n",
    "\n",
    "# def remove_gibberish(text):\n",
    "    \n",
    "#     from nltk.corpus import words\n",
    "    \n",
    "#     test = text.split(\" \")\n",
    "#     # final = []\n",
    "#     start=0\n",
    "#     end=len(test)\n",
    "#     for i in range(len(test)):\n",
    "#         x = test[i]\n",
    "#         if x in words.words() or x.isdigit():\n",
    "#             start=i\n",
    "#             break\n",
    "\n",
    "#     for i in range(len(test)-1,-1,-1):\n",
    "#         x = test[i]\n",
    "#         if x in words.words() or x.isdigit():\n",
    "#             end=i\n",
    "#             break\n",
    "\n",
    "#     return \" \".join(test[start:end+1])\n",
    "\n",
    "def get_all_text(gcloud_data):\n",
    "    # Returns a Text Array of the OCR data going throught the Gcloud Vision API Response\"\"\"\n",
    "    all_texts = []\n",
    "    for textAnnotations in gcloud_data['responses']:\n",
    "        text_raw = textAnnotations['fullTextAnnotation']['text'].encode('ascii','ignore')\n",
    "        text = text_raw.replace(\"\\n\", \" \")\n",
    "        # text_cleaned = remove_gibberish(text)\n",
    "        \n",
    "        all_texts.append(text.strip())\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "def create_note_from_highlight(authToken,image_files, note_content, ocr=False, notetitle=''):\n",
    "    from time import gmtime, strftime\n",
    "\n",
    "    if notetitle=='':\n",
    "        curr_time = strftime(\"%Y-%m-%d\", gmtime())\n",
    "        notetitle=\"digilight {curr_time}\".format(curr_time=curr_time)\n",
    "    \n",
    "    if ocr:\n",
    "        all_texts=[]\n",
    "        for image_file in image_files:\n",
    "            json_data, text =google_ocr_img(image_file)\n",
    "            all_texts.append(text)\n",
    "\n",
    "        note_content = \"\\n---------------------\\n\".join(all_texts)\n",
    "    # json_data=open(\"jsons/api_result.json\").read()\n",
    "\n",
    "    # data = json.loads(json_data)\n",
    "    # all_texts = get_all_text(data)\n",
    "\n",
    "    client = get_evernote_client(authToken)\n",
    "    userStore = client.get_user_store()\n",
    "    user = userStore.getUser()\n",
    "\n",
    "\n",
    "    noteStore = client.get_note_store()\n",
    "    notebooks = noteStore.listNotebooks()\n",
    "    found=False\n",
    "    for n in notebooks:\n",
    "        if n.name=='Digilights':\n",
    "            parentNotebook=n\n",
    "            found=True\n",
    "\n",
    "    if not found:\n",
    "        ourNotebook = Types.Notebook()\n",
    "        ourNotebook.name = \"Digilights\"\n",
    "        parentNotebook = noteStore.createNotebook(ourNotebook)\n",
    "\n",
    "\n",
    "    try:\n",
    "        resources = create_en_resource(image_files)\n",
    "        note = makeNote(authToken, noteStore, notetitle, note_content,\n",
    "                 parentNotebook=parentNotebook, resources=resources)\n",
    "    except:\n",
    "        return \"ERROR: Couldnt make evernote.\", note_content\n",
    "    \n",
    "    msg = note.title + \" created in \" + parentNotebook.name + \"!\"\n",
    "    return msg, note_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING: Post OCR Contour experiment...\n",
      "Edge Points: (61, 688) (1158, 687) (1135, 631) (352, 724)\n",
      "Centroid: ( 662 , 679 )\n",
      "AREA of Contour: 60962.5\n",
      "Epsilon 4.41504910553\n",
      "contoured_hammad_badone.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"STARTING: Post OCR Contour experiment...\"\n",
    "img_path = \"sample_images/hammad_badone.jpg\"\n",
    "c = contour_img(img_path, thresh=100, std_dev=2, hsv_lower=[22, 30, 30], hsv_upper=[45, 255, 255])\n",
    "print c\n",
    "cv2.imshow(\"contoured\",cv2.imread(c))\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'responses']\n"
     ]
    }
   ],
   "source": [
    "api_res, text = google_ocr_img([img_path])\n",
    "print api_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'textAnnotations', u'fullTextAnnotation']\n",
      "TOTAL WORDS OCR-ed: 517\n"
     ]
    }
   ],
   "source": [
    "res = api_res['responses'][0]\n",
    "print res.keys()\n",
    "txt_ann = res['textAnnotations']\n",
    "fulltxt_ann = res['fullTextAnnotation']\n",
    "word_objects = []\n",
    "for x in range(len(txt_ann)):\n",
    "    obj = txt_ann[x]\n",
    "    if ' ' not in obj['description']:\n",
    "        word_objects.append(obj)\n",
    "\n",
    "print \"TOTAL WORDS OCR-ed:\", len(word_objects)\n",
    "# print \"FIRST WORD BOUNDING BOX:\", word_objects[0]['boundingPoly']\n",
    "# print \"LAST WORD BOUNDING BOX:\", word_objects[-1]['boundingPoly']\n",
    "# print \" \".join([x['description'] for x in word_objects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599L, 1200L, 3L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(img_path)\n",
    "print image.shape\n",
    "hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "hsv_lower=[22, 30, 30]\n",
    "hsv_upper=[45, 255, 255]\n",
    "HSV_lower = np.array(hsv_lower, np.uint8)  # Lower HSV value\n",
    "HSV_upper = np.array(hsv_upper, np.uint8)  # Upper HSV value\n",
    "\n",
    "frame_threshed = cv2.inRange(hsv_img, HSV_lower, HSV_upper)\n",
    "cv2.imshow('framethres', frame_threshed)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def return_bigger_mask_if_intersect_or_mask2(mask1, mask2, threshold=0.2):\n",
    "    intersectarea = np.sum(mask1[mask2==255])\n",
    "    area1 = np.sum(mask1)\n",
    "    area2 = np.sum(mask2)\n",
    "    print \"AREA1:\", area1, \"AREA2:\", area2, \"INTERSECT_AREA:\", intersectarea\n",
    "    if intersectarea>threshold*area1 or intersectarea>threshold*area2:\n",
    "        bigger_mask=mask1\n",
    "        if area2>area1:\n",
    "            bigger_mask=mask2\n",
    "        return True, bigger_mask\n",
    "    return False, mask2\n",
    "\n",
    "def swap_on_intersect(mask1, mask2, threshold=0.2):\n",
    "    intersectarea = np.sum(mask1[mask2==255])/255\n",
    "    area1 = np.sum(mask1)/255\n",
    "    area2 = np.sum(mask2)/255\n",
    "    if intersectarea>threshold*area1 or intersectarea>threshold*area2:\n",
    "        print \"AREA1:\", area1, \"AREA2:\", area2, \"INTERSECT_AREA:\", intersectarea\n",
    "        if area2<area1:\n",
    "            return True, False\n",
    "        else:\n",
    "            return True, True\n",
    "    return False, False\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599L, 1200L, 3L)\n",
      "--- WORD SEL END: 14.9460000992 seconds ---\n",
      "are more flexible than parallel sections, because parallel sections constrain exactly how many threads are supposed to run, and there is also always a join at the end of the parallel section. On the other hand, the OpenMP runtime can assign any task to any thread that' s running. Tasks therefore have lower overhead.\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(img_path)\n",
    "import time\n",
    "beginning = time.time()\n",
    "\n",
    "word_sel_thres = 5\n",
    "print image.shape\n",
    "selected_obj = []\n",
    "hili_text = []\n",
    "for x in range(len(word_objects[:])):\n",
    "    obj=word_objects[x]\n",
    "    word_objects[x]['sel'] = False\n",
    "    try:\n",
    "        bounding_box = obj['boundingPoly']\n",
    "        box_points = [(p['x'],p['y']) for p in bounding_box['vertices'] ]\n",
    "        box_points = np.array(box_points)\n",
    "    except:\n",
    "        continue\n",
    "    mask = np.zeros((hsv_img.shape[0], hsv_img.shape[1]))\n",
    "    out = np.zeros((hsv_img.shape[0], hsv_img.shape[1]))\n",
    "    cv2.fillConvexPoly(mask, box_points, (255, 255, 255))\n",
    "    word_objects[x]['polymask'] = mask\n",
    "    avg = np.sum(frame_threshed[mask==255])/255.0\n",
    "    mask_area = np.sum(mask)/255.0\n",
    "    \n",
    "    if avg>0.5*mask_area:\n",
    "#         print \"POTENTIAL WORD at \", x, \"with sel size:\",len(selected_obj)\n",
    "        start_time=time.time()\n",
    "        #check if the mask intersects with other masks\n",
    "        intersect_flag=False\n",
    "        for y in selected_obj:\n",
    "            if abs(x-y)==1 or abs(x-y)>15:\n",
    "                continue\n",
    "            res, swap = swap_on_intersect(mask, word_objects[y]['polymask'], threshold=0.2)\n",
    "#             print \"--- INTERSECT CHECK: %s seconds ---\" % (time.time() - start_time)\n",
    "            if res:\n",
    "                intersect_flag=True\n",
    "                print \"INTERSECTED:\"\n",
    "                print x, word_objects[x]['description']\n",
    "                print y, word_objects[y]['description']\n",
    "                if len(word_objects[y]['description'])<len(word_objects[x]['description']):\n",
    "                    word_objects[y]['description'] = word_objects[x]['description'] \n",
    "                break\n",
    "#         print \"--- WORD ADD: %s seconds ---\" % (time.time() - start_time)\n",
    "        if intersect_flag:\n",
    "            continue\n",
    "        \n",
    "        word_objects[x]['sel'] = True\n",
    "        selected_obj.append(x)\n",
    "        look_back = max(x-word_sel_thres,0)\n",
    "        if word_objects[look_back]['sel']:\n",
    "            for i in range(look_back+1,x):\n",
    "                if not word_objects[i]['sel']:\n",
    "                    word_objects[i]['sel']=True\n",
    "                    selected_obj.append(i)\n",
    "                    \n",
    "print \"--- WORD SEL END: %s seconds ---\" % (time.time() - beginning)\n",
    "        \n",
    "#second round to\n",
    "for obj in word_objects:\n",
    "    try:\n",
    "        if obj['sel']:\n",
    "            bounding_box = obj['boundingPoly']\n",
    "            box_points = [(p['x'],p['y']) for p in bounding_box['vertices'] ]\n",
    "            box_points = np.array(box_points)\n",
    "            hili_text.append(obj['description'])\n",
    "            cv2.drawContours(image, [box_points], 0, (0, 0, 50), 1)\n",
    "    except:\n",
    "        continue\n",
    "cv2.imwrite(\"POC-hammad.png\",image)\n",
    "print_text = [hili_text[0]]\n",
    "for w in hili_text[1:]:\n",
    "    if w not in ['?','.',',',':',';','(',')',\"'\",'\"',\"-\",\"_\"]:\n",
    "        print_text.append(\" \"+w)\n",
    "    else:\n",
    "        print_text.append(w)\n",
    "        \n",
    "print \"\".join(print_text)\n",
    "# cv2.imshow('Output', image)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "        \n",
    "    # Now show the image\n",
    "#     cv2.imshow('Output', res)\n",
    "#     cv2.waitKey(0)\n",
    "#     word_img = image[]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
